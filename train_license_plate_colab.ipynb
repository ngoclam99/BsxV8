{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# üöó Nh·∫≠n Di·ªán Bi·ªÉn S·ªë Xe v·ªõi YOLOv8\n",
    "## Training tr√™n Google Colab\n",
    "\n",
    "Notebook n√†y s·∫Ω train model YOLOv8 ƒë·ªÉ nh·∫≠n di·ªán bi·ªÉn s·ªë xe s·ª≠ d·ª•ng dataset c·ªßa b·∫°n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# C√†i ƒë·∫∑t ultralytics v√† c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
    "!pip install ultralytics\n",
    "!pip install roboflow\n",
    "\n",
    "# Import th∆∞ vi·ªán\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "from ultralytics import YOLO\n",
    "from IPython.display import Image, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount_drive"
   },
   "source": [
    "## 2. K·∫øt n·ªëi Google Drive (T√πy ch·ªçn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive ƒë·ªÉ l∆∞u tr·ªØ dataset v√† k·∫øt qu·∫£\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c l√†m vi·ªác\n",
    "work_dir = '/content/license_plate_detection'\n",
    "os.makedirs(work_dir, exist_ok=True)\n",
    "os.chdir(work_dir)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_data"
   },
   "source": [
    "## 3. Upload Dataset\n",
    "### C√°ch 1: Clone t·ª´ GitHub (Khuy·∫øn ngh·ªã)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "github_clone"
   },
   "outputs": [],
   "source": [
    "# Clone repository t·ª´ GitHub (thay YOUR_GITHUB_USERNAME v√† YOUR_REPO_NAME)\n",
    "# !git clone https://github.com/YOUR_GITHUB_USERNAME/NhanDienBSX.git\n",
    "# os.chdir('NhanDienBSX')\n",
    "\n",
    "# Ho·∫∑c download t·ª´ link tr·ª±c ti·∫øp\n",
    "print(\"N·∫øu b·∫°n c√≥ GitHub repo, uncomment v√† s·ª≠a l·∫°i d√≤ng git clone ·ªü tr√™n\")\n",
    "print(\"Ho·∫∑c s·ª≠ d·ª•ng c√°ch upload file ·ªü cell ti·∫øp theo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_manual"
   },
   "source": [
    "### C√°ch 2: Upload th·ªß c√¥ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "manual_upload"
   },
   "outputs": [],
   "source": [
    "# Upload files t·ª´ m√°y t√≠nh\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Upload file mydata.yaml:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(\"\\nUpload dataset folder (n√©n th√†nh .zip tr∆∞·ªõc):\")\n",
    "uploaded_dataset = files.upload()\n",
    "\n",
    "# Gi·∫£i n√©n dataset n·∫øu c·∫ßn\n",
    "import zipfile\n",
    "for filename in uploaded_dataset.keys():\n",
    "    if filename.endswith('.zip'):\n",
    "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print(f\"ƒê√£ gi·∫£i n√©n {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_yaml"
   },
   "source": [
    "## 4. T·∫°o file c·∫•u h√¨nh dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yaml_config"
   },
   "outputs": [],
   "source": [
    "# T·∫°o file mydata.yaml v·ªõi ƒë∆∞·ªùng d·∫´n tuy·ªát ƒë·ªëi\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "dataset_path = os.path.join(current_dir, 'dataset')\n",
    "\n",
    "yaml_content = f\"\"\"\n",
    "# Dataset configuration for License Plate Detection\n",
    "path: {dataset_path}  # dataset root dir (absolute path)\n",
    "train: images/train  # train images (relative to 'path')\n",
    "val: images/val  # val images (relative to 'path')\n",
    "test:  # test images (optional)\n",
    "\n",
    "# Classes\n",
    "names:\n",
    "  0: Bien_So\n",
    "\n",
    "# Number of classes\n",
    "nc: 1\n",
    "\"\"\"\n",
    "\n",
    "with open('mydata.yaml', 'w', encoding='utf-8') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(f\"ƒê√£ t·∫°o file mydata.yaml v·ªõi ƒë∆∞·ªùng d·∫´n: {dataset_path}\")\n",
    "print(\"N·ªôi dung file:\")\n",
    "with open('mydata.yaml', 'r', encoding='utf-8') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_dataset"
   },
   "source": [
    "## 5. Ki·ªÉm tra c·∫•u tr√∫c dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dataset_check"
   },
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c\n",
    "def check_dataset_structure():\n",
    "    required_dirs = [\n",
    "        'dataset/images/train',\n",
    "        'dataset/images/val',\n",
    "        'dataset/labels/train',\n",
    "        'dataset/labels/val'\n",
    "    ]\n",
    "    \n",
    "    print(\"Ki·ªÉm tra c·∫•u tr√∫c dataset:\")\n",
    "    all_good = True\n",
    "    for dir_path in required_dirs:\n",
    "        if os.path.exists(dir_path):\n",
    "            files = [f for f in os.listdir(dir_path) if f.endswith(('.jpg', '.png', '.jpeg', '.txt'))]\n",
    "            file_count = len(files)\n",
    "            print(f\"‚úÖ {dir_path}: {file_count} files\")\n",
    "            if file_count == 0:\n",
    "                print(f\"   ‚ö†Ô∏è  Th∆∞ m·ª•c tr·ªëng!\")\n",
    "                all_good = False\n",
    "        else:\n",
    "            print(f\"‚ùå {dir_path}: Kh√¥ng t·ªìn t·∫°i\")\n",
    "            all_good = False\n",
    "    \n",
    "    # Ki·ªÉm tra t∆∞∆°ng ·ª©ng gi·ªØa images v√† labels\n",
    "    train_images = [f.split('.')[0] for f in os.listdir('dataset/images/train') if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    train_labels = [f.split('.')[0] for f in os.listdir('dataset/labels/train') if f.endswith('.txt')]\n",
    "    val_images = [f.split('.')[0] for f in os.listdir('dataset/images/val') if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    val_labels = [f.split('.')[0] for f in os.listdir('dataset/labels/val') if f.endswith('.txt')]\n",
    "    \n",
    "    print(f\"\\nüìä Th·ªëng k√™ dataset:\")\n",
    "    print(f\"   Train: {len(train_images)} images, {len(train_labels)} labels\")\n",
    "    print(f\"   Val: {len(val_images)} images, {len(val_labels)} labels\")\n",
    "    \n",
    "    if len(train_images) != len(train_labels):\n",
    "        print(f\"   ‚ö†Ô∏è  Train: S·ªë l∆∞·ª£ng images v√† labels kh√¥ng kh·ªõp!\")\n",
    "        all_good = False\n",
    "    if len(val_images) != len(val_labels):\n",
    "        print(f\"   ‚ö†Ô∏è  Val: S·ªë l∆∞·ª£ng images v√† labels kh√¥ng kh·ªõp!\")\n",
    "        all_good = False\n",
    "    \n",
    "    if all_good:\n",
    "        print(\"\\nüéâ Dataset ƒë√£ s·∫µn s√†ng cho training!\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Dataset c√≥ v·∫•n ƒë·ªÅ, c·∫ßn ki·ªÉm tra l·∫°i!\")\n",
    "    \n",
    "    return all_good\n",
    "\n",
    "dataset_ready = check_dataset_structure()\n",
    "\n",
    "# Hi·ªÉn th·ªã c·∫•u tr√∫c th∆∞ m·ª•c\n",
    "print(\"\\nüìÅ C·∫•u tr√∫c th∆∞ m·ª•c:\")\n",
    "!find . -type d -name \"*\" | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_model"
   },
   "source": [
    "## 6. Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "training"
   },
   "outputs": [],
   "source": [
    "# Ki·ªÉm tra dataset tr∆∞·ªõc khi training\n",
    "if not dataset_ready:\n",
    "    print(\"‚ùå Dataset ch∆∞a s·∫µn s√†ng! Vui l√≤ng ki·ªÉm tra l·∫°i.\")\n",
    "    raise Exception(\"Dataset kh√¥ng h·ª£p l·ªá\")\n",
    "\n",
    "# Ki·ªÉm tra file mydata.yaml\n",
    "if not os.path.exists('mydata.yaml'):\n",
    "    print(\"‚ùå File mydata.yaml kh√¥ng t·ªìn t·∫°i!\")\n",
    "    raise Exception(\"File c·∫•u h√¨nh dataset kh√¥ng t·ªìn t·∫°i\")\n",
    "\n",
    "# Load pretrained YOLO model\n",
    "print(\"üì• Downloading pretrained model...\")\n",
    "model = YOLO(\"yolov8x.pt\")  # ho·∫∑c yolov8n.pt, yolov8s.pt, yolov8m.pt, yolov8l.pt\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 50\n",
    "IMG_SIZE = 640\n",
    "BATCH_SIZE = 16  # Gi·∫£m n·∫øu b·ªã out of memory\n",
    "\n",
    "print(f\"\\nüöÄ B·∫Øt ƒë·∫ßu training v·ªõi {EPOCHS} epochs...\")\n",
    "print(f\"üì± Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"üñºÔ∏è  Image size: {IMG_SIZE}\")\n",
    "print(f\"üì¶ Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "# Validate dataset tr∆∞·ªõc khi train\n",
    "try:\n",
    "    print(\"\\nüîç Validating dataset...\")\n",
    "    model.val(data=\"mydata.yaml\", split='val')\n",
    "    print(\"‚úÖ Dataset validation th√†nh c√¥ng!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Dataset validation failed: {e}\")\n",
    "    print(\"Ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n v√† c·∫•u tr√∫c dataset\")\n",
    "    raise\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nüèãÔ∏è B·∫Øt ƒë·∫ßu training...\")\n",
    "results = model.train(\n",
    "    data=\"mydata.yaml\",\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMG_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    device=0 if torch.cuda.is_available() else 'cpu',\n",
    "    project='/content/runs',\n",
    "    name='license_plate_detection',\n",
    "    save=True,\n",
    "    save_period=10,  # L∆∞u checkpoint m·ªói 10 epochs\n",
    "    patience=20,  # Early stopping\n",
    "    verbose=True,\n",
    "    plots=True  # T·∫°o plots\n",
    ")\n",
    "\n",
    "print(\"\\nüéâ Training ho√†n th√†nh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "results"
   },
   "source": [
    "## 7. Xem k·∫øt qu·∫£ training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view_results"
   },
   "outputs": [],
   "source": [
    "# Hi·ªÉn th·ªã k·∫øt qu·∫£ training\n",
    "results_dir = '/content/runs/license_plate_detection'\n",
    "\n",
    "# T√¨m th∆∞ m·ª•c k·∫øt qu·∫£ m·ªõi nh·∫•t\n",
    "import glob\n",
    "result_folders = glob.glob('/content/runs/license_plate_detection*')\n",
    "if result_folders:\n",
    "    latest_folder = max(result_folders, key=os.path.getctime)\n",
    "    print(f\"Th∆∞ m·ª•c k·∫øt qu·∫£: {latest_folder}\")\n",
    "    \n",
    "    # Hi·ªÉn th·ªã c√°c bi·ªÉu ƒë·ªì training\n",
    "    plots = ['results.png', 'confusion_matrix.png', 'train_batch0.png', 'val_batch0_pred.png']\n",
    "    \n",
    "    for plot in plots:\n",
    "        plot_path = os.path.join(latest_folder, plot)\n",
    "        if os.path.exists(plot_path):\n",
    "            print(f\"\\nüìä {plot}:\")\n",
    "            display(Image(plot_path))\n",
    "        else:\n",
    "            print(f\"‚ùå Kh√¥ng t√¨m th·∫•y {plot}\")\n",
    "else:\n",
    "    print(\"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c k·∫øt qu·∫£\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test_model"
   },
   "source": [
    "## 8. Test model v·ªõi ·∫£nh m·∫´u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "testing"
   },
   "outputs": [],
   "source": [
    "# Load model ƒë√£ train\n",
    "best_model_path = os.path.join(latest_folder, 'weights', 'best.pt')\n",
    "if os.path.exists(best_model_path):\n",
    "    trained_model = YOLO(best_model_path)\n",
    "    print(f\"‚úÖ ƒê√£ load model: {best_model_path}\")\n",
    "    \n",
    "    # Test v·ªõi ·∫£nh validation\n",
    "    val_images = glob.glob('dataset/images/val/*.jpg') + glob.glob('dataset/images/val/*.png')\n",
    "    \n",
    "    if val_images:\n",
    "        test_image = val_images[0]\n",
    "        print(f\"\\nüîç Test v·ªõi ·∫£nh: {test_image}\")\n",
    "        \n",
    "        # Predict\n",
    "        results = trained_model(test_image)\n",
    "        \n",
    "        # Hi·ªÉn th·ªã k·∫øt qu·∫£\n",
    "        results[0].show()\n",
    "        \n",
    "        # L∆∞u k·∫øt qu·∫£\n",
    "        results[0].save('prediction_result.jpg')\n",
    "        display(Image('prediction_result.jpg'))\n",
    "    else:\n",
    "        print(\"‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh test\")\n",
    "else:\n",
    "    print(\"‚ùå Kh√¥ng t√¨m th·∫•y model ƒë√£ train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 9. Download model v√† k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_results"
   },
   "outputs": [],
   "source": [
    "# N√©n v√† download k·∫øt qu·∫£\n",
    "import shutil\n",
    "\n",
    "if result_folders:\n",
    "    # T·∫°o file zip ch·ª©a k·∫øt qu·∫£\n",
    "    shutil.make_archive('license_plate_model', 'zip', latest_folder)\n",
    "    \n",
    "    # Download file zip\n",
    "    files.download('license_plate_model.zip')\n",
    "    \n",
    "    # Download ri√™ng file model t·ªët nh·∫•t\n",
    "    if os.path.exists(best_model_path):\n",
    "        shutil.copy(best_model_path, 'best_license_plate_model.pt')\n",
    "        files.download('best_license_plate_model.pt')\n",
    "    \n",
    "    print(\"‚úÖ ƒê√£ download model v√† k·∫øt qu·∫£ training!\")\n",
    "    print(f\"üìÅ Model path: {best_model_path}\")\n",
    "    print(f\"üìä Results folder: {latest_folder}\")\n",
    "else:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_to_drive"
   },
   "source": [
    "## 10. L∆∞u v√†o Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_drive"
   },
   "outputs": [],
   "source": [
    "# L∆∞u k·∫øt qu·∫£ v√†o Google Drive\n",
    "drive_save_path = '/content/drive/MyDrive/license_plate_detection_results'\n",
    "os.makedirs(drive_save_path, exist_ok=True)\n",
    "\n",
    "if result_folders:\n",
    "    # Copy to√†n b·ªô k·∫øt qu·∫£\n",
    "    shutil.copytree(latest_folder, os.path.join(drive_save_path, 'training_results'), dirs_exist_ok=True)\n",
    "    \n",
    "    # Copy model t·ªët nh·∫•t\n",
    "    if os.path.exists(best_model_path):\n",
    "        shutil.copy(best_model_path, os.path.join(drive_save_path, 'best_model.pt'))\n",
    "    \n",
    "    print(f\"‚úÖ ƒê√£ l∆∞u k·∫øt qu·∫£ v√†o Google Drive: {drive_save_path}\")\n",
    "else:\n",
    "    print(\"‚ùå Kh√¥ng c√≥ k·∫øt qu·∫£ ƒë·ªÉ l∆∞u\")\n",
    "\n",
    "print(\"\\nüéâ Ho√†n th√†nh! Model ƒë√£ ƒë∆∞·ª£c train v√† l∆∞u tr·ªØ.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
